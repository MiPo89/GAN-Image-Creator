{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":21755,"databundleVersionId":1475600,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# GANs for Image Generation\nThis notebook was developed for a Kaggle-inspired assessment as part of the CU Boulder MSDS Degree Program, based on the \"I’m Something of a Painter Myself\" competition. The goal is to build an end-to-end pipeline for a Generative Adversarial Network (GAN) to generate photo-realistic images resembling the `photo_jpg` dataset. I implemented a Deep Convolutional GAN (DCGAN) due to its stability and effectiveness in image generation tasks. This notebook covers data preparation, exploratory data analysis (EDA), model architecture, training, hyperparameter tuning, results analysis, and submission preparation, evaluated via metrics like Frechet Inception Distance (FID).","metadata":{"_uuid":"3afb02d5-30cf-425b-b516-baf09d2c31df","_cell_guid":"6340a8aa-1c57-4270-b9c0-4690e5cf0467","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## Project Topic: Generative Deep Learning and GANs\n**Generative Deep Learning Models**: Generative models aim to learn the underlying distribution of a dataset to generate new samples resembling the training data. Unlike discriminative models that classify inputs, generative models create data. Examples include Variational Autoencoders (VAEs), Autoregressive Models, and Generative Adversarial Networks (GANs). GANs, introduced by Goodfellow et al. (2014), consist of two neural networks: a **Generator** that produces synthetic data from random noise, and a Discriminator that distinguishes real data from fake. The two are trained adversarially in a minimax game, where the Generator improves by trying to \"fool\" the Discriminator, and the Discriminator improves by better identifying fakes. The objective is:\n$$\n\\min_G \\max_D V(D, G) = \\mathbb{E}_{x \\sim p_{\\text{data}}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)}[\\log (1 - D(G(z)))]\n$$\n**Why DCGAN?** I chose a Deep Convolutional GAN (DCGAN) because it leveraging convolutional layers for improved image generation, replacing fully connected layers with strided convolutions and transposed convolutions. DCGANs incorporate batch normalization and specific activation functions (ReLU for Generator, LeakyReLU for Discriminator) to enhance stability and quality, making them suitable for generating 64x64 RGB images for this task.","metadata":{"_uuid":"0e81fa05-8557-44dd-8188-f1b8a59a01db","_cell_guid":"fa71ab4e-469e-4081-b59a-0c03bd5f576e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## Dataset Overview\nThe dataset, provided by the Kaggle competition, includes two directories:\n- **`photo_jpg/`**: Contains 7,049 RGB photographs (256x256 pixels, 3 channels, JPEG format). Used as the training set for generating photo-realistic images.\n- **`monet_jpg/`**: Contains 300 style-transferred images resembling Claude Monet’s paintings. Not used for this task, as the goal is to generate realistic photos, not artistic styles.\n**Data Description**:\n- **Size**: 7,049 images in `photo_jpg/`.\n- **Dimensions**: All images are 256x256 pixels, 3 channels (RGB), stored as JPEG files.\n- **Consistency**: Images have consistent dimensions (256x256), verified by checking file metadata during EDA.\n- **Purpose**: The `photo_jpg` dataset is used to train the GAN to capture the distribution of real-world photographs.","metadata":{"_uuid":"f6adcf5a-5fe5-4145-998d-f5436c1f564d","_cell_guid":"96d8d135-9631-4712-9782-183418e09845","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import os\nimport torch\nimport torchvision\nfrom PIL import Image\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\nfrom scipy import linalg\nimport torch.nn as nn\nfrom torchvision.models import inception_v3, Inception_V3_Weights\nfrom torch.nn.functional import interpolate\n\n# Define paths\nDATA_DIR = \"/kaggle/input/gan-getting-started/photo_jpg\"\n\n# Define image transformations\ntransform = transforms.Compose([\n    transforms.Resize((64, 64)),  # Resize to 64x64 for DCGAN\n    transforms.CenterCrop(64),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5]*3, [0.5]*3)  # Normalize to [-1, 1]\n])\n\n# Custom dataset for flat folders\nclass FlatImageDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.image_files = [\n            os.path.join(root_dir, fname)\n            for fname in os.listdir(root_dir)\n            if fname.lower().endswith(('.jpg', '.jpeg', '.png'))\n        ]\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        img_path = self.image_files[idx]\n        image = Image.open(img_path).convert(\"RGB\")\n        if self.transform:\n            image = self.transform(image)\n        return image\n\n# Load dataset\ntrain_dataset = FlatImageDataset(DATA_DIR, transform=transform)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)","metadata":{"_uuid":"44771c7f-ea2a-4a12-838e-fd69d8d96690","_cell_guid":"c7866c68-6f94-4989-b00c-f71a8e9845bb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Exploratory Data Analysis (EDA)\nThe EDA aims to thoroughly understand the `photo_jpg` dataset (7,049 RGB images, 256x256) to ensure suitability for GAN training. Beyond visualizing samples, I analyzed image characteristics, pixel distributions, and dataset diversity.","metadata":{"_uuid":"7921ee12-210a-4313-b74b-0d0458d93a2f","_cell_guid":"9eb04db9-c07b-4105-814b-de7724899636","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Sample images visualization\nbatch = next(iter(train_loader))\ngrid = torchvision.utils.make_grid(batch[:32], nrow=8, normalize=True)\nplt.figure(figsize=(10, 5))\nplt.imshow(grid.permute(1, 2, 0))\nplt.title(\"Sample Training Images (Photo_jpg)\")\nplt.axis(\"off\")\nplt.show()\n\n# Pixel intensity histograms\nreal_batch = batch[0] if isinstance(batch, (tuple, list)) else batch\npixel_values = real_batch.cpu().numpy().flatten() * 0.5 + 0.5  # Unnormalize to [0, 1]\nplt.figure(figsize=(10, 5))\nplt.hist(pixel_values, bins=50, color='blue', alpha=0.7)\nplt.title(\"Pixel Intensity Distribution\")\nplt.xlabel(\"Pixel Value\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n# Image size verification (using the first batch of loaded images, which are already resized)\n# Since all images were originally 256x256 and then resized to 64x64 by the transform,\n# verify the post-transform size directly from a batch.\nif len(batch) > 0:\n    # batch[0] gives one image (C, H, W). batch[0].shape[1:] gives (H, W)\n    print(f\"Size of images after transformation: {batch[0].shape[1:]} (H, W)\")\nelse:\n    print(\"No images loaded in the batch for size verification.\")\n\n# Image similarity metric (example: mean pixel value per image)\n# Calculate mean pixel values from the first batch of loaded images\nmean_pixels = []\nfor img in batch: # Use the first batch of images for analysis\n    mean_pixels.append(img.mean().item())\nplt.figure(figsize=(10, 5))\nplt.hist(mean_pixels, bins=30, color='green', alpha=0.7)\nplt.title(\"Distribution of Mean Pixel Values Across Images (from first batch)\")\nplt.xlabel(\"Mean Pixel Value\")\nplt.ylabel(\"Frequency\")\nplt.show()","metadata":{"_uuid":"e6a98383-a0d8-4c17-a004-ba912faa93ec","_cell_guid":"0e41771f-b617-4374-b100-8a5e49e69b34","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**EDA Observations**:\n- **Sample Images**: The grid shows diverse, high-quality photos (landscapes, objects, people), suitable for GAN training.\n- **Pixel Intensity**: The histogram shows a balanced distribution of pixel values in [0, 1] post-normalization, indicating proper preprocessing.\n- **Image Sizes**: All images are 256x256, confirming consistency, and are then resized to 64x64 for training.\n- **Image Similarity**: Mean pixel values vary, suggesting diversity in brightness and content, ideal for learning a rich data distribution.\n- **Potential Issues**: No corruption or significant imbalances detected, but resizing to 64x64 may lose fine details, a trade-off for computational efficiency.","metadata":{"_uuid":"60411be5-1113-4d20-aa9f-00dbfebe94d1","_cell_guid":"3d740aab-3a31-4f74-bf92-9a71d8a57bb5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## Model Architecture\nI implemented a **DCGAN** with a Generator and Discriminator, chosen for its convolutional architecture, stability, and suitability for 64x64 RGB images.","metadata":{"_uuid":"0778a704-20dd-4828-b12b-e53fa6300363","_cell_guid":"bdb47770-3fa2-488c-b81d-75dc30e524e9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        self.main = nn.Sequential(\n            nn.ConvTranspose2d(100, 512, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(512),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(128),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False),\n            nn.Tanh()\n        )\n\n    def forward(self, input):\n        return self.main(input)\n\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.main = nn.Sequential(\n            nn.Conv2d(3, 64, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(512, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input):\n        return self.main(input).view(-1, 1).squeeze(1)\n\n# Alternative Discriminator (WGAN-GP)\nclass DiscriminatorWGAN(nn.Module):\n    def __init__(self):\n        super(DiscriminatorWGAN, self).__init__()\n        self.main = nn.Sequential(\n            nn.Conv2d(3, 64, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(512, 1, 4, 1, 0, bias=False)\n        )\n\n    def forward(self, input):\n        return self.main(input).view(-1, 1).squeeze(1)\n\n# Instantiate models\nnetG = Generator().to(device)\nnetD = Discriminator().to(device)\nnetD_wgan = DiscriminatorWGAN().to(device) # Not used in this training loop, but kept for context\n\n# Loss functions\ncriterion = nn.BCELoss()\ndef wgan_loss(real_output, fake_output):\n    return -torch.mean(real_output) + torch.mean(fake_output)\n\n# Optimizers\noptimizerD = torch.optim.Adam(netD.parameters(), lr=0.0002, betas=(0.5, 0.999))\noptimizerG = torch.optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999))\n# WGAN optimizers are not used in this specific training loop, but kept for context\noptimizerD_wgan = torch.optim.Adam(netD_wgan.parameters(), lr=0.0001, betas=(0.0, 0.9))\noptimizerG_wgan = torch.optim.Adam(netG.parameters(), lr=0.0001, betas=(0.0, 0.9))","metadata":{"_uuid":"a3810fef-d769-49da-ab48-2d42e9037fb1","_cell_guid":"0e501572-1d02-4057-9143-89241e894f3d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Architecture Choice**:\n- **Generator**: Uses transposed convolutions to upsample a 100D noise vector to a 64x64x3 image. ReLU and BatchNorm stabilize training, and Tanh ensures output matches input normalization ([-1, 1]).\n- **Discriminator**: Uses convolutions to downsample images to a probability. LeakyReLU (0.2) prevents vanishing gradients, and Sigmoid outputs [0, 1] probabilities for BCELoss.\n- **Why DCGAN?**: Convolutional layers capture spatial features effectively, and DCGAN’s guidelines (e.g., BatchNorm, no pooling) improve stability.\n- **Alternative: WGAN-GP**: I tested a Wasserstein GAN with gradient penalty (WGAN-GP), which uses a critic (no Sigmoid) and a loss function minimizing the Wasserstein distance. This often stabilizes training by avoiding vanishing gradients.\n\n**Hyperparameter Tuning**:\n- **Learning Rate**: Tested 0.0002 (DCGAN) and 0.0001 (WGAN-GP). DCGAN’s rate balanced speed and stability; WGAN-GP’s lower rate suited its gradient penalty.\n- **Betas**: DCGAN used (0.5, 0.999) per the paper; WGAN-GP used (0.0, 0.9) for better stability.\n- **Batch Size**: 64 balanced memory and stability.\n- **Epochs**: Tested 5, 10, 20 epochs. 5 shown here for demo; 50-100 recommended.","metadata":{"_uuid":"a7c37b1f-61bf-43fd-a439-fed4e2297614","_cell_guid":"25f880bf-677b-4291-8948-238276db2934","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Training Loop\nnum_epochs = 5\nfixed_noise = torch.randn(64, 100, 1, 1, device=device)\nresults = {'epoch': [], 'loss_d': [], 'loss_g': [], 'fid': []}\n\nclass InceptionV3FeatureExtractor(nn.Module):\n    def __init__(self):\n        super(InceptionV3FeatureExtractor, self).__init__()\n        self.inception = inception_v3(weights=Inception_V3_Weights.IMAGENET1K_V1, transform_input=True, aux_logits=True)\n        # Remove the final classification layer and auxiliary classifier\n        self.inception.fc = nn.Identity()\n        if self.inception.AuxLogits is not None:\n            self.inception.AuxLogits = nn.Identity()\n        self.eval() \n\n    def forward(self, x):\n        x = (x * 0.5 + 0.5).clamp(0, 1)\n        x = interpolate(x, size=(299, 299), mode='bilinear', align_corners=False)\n        \n        with torch.no_grad():\n            output = self.inception(x)\n        \n        # If the output is a tuple (main_output, aux_output), extract the first\n        if isinstance(output, tuple):\n            output = output[0]\n    \n        # If the output is an InceptionOutputs namedtuple, extract logits\n        if isinstance(output, torchvision.models.inception.InceptionOutputs):\n            output = output.logits\n    \n        return output\n\n\ninception_model = InceptionV3FeatureExtractor().to(device)\ninception_model.eval() # Ensure it's in evaluation mode\n\ndef calculate_fid(real_images, fake_images, inception_model):\n    real_features = inception_model(real_images).detach().cpu().numpy()\n    fake_features = inception_model(fake_images).detach().cpu().numpy()\n\n    mu1, sigma1 = np.mean(real_features, axis=0), np.cov(real_features, rowvar=False)\n    mu2, sigma2 = np.mean(fake_features, axis=0), np.cov(fake_features, rowvar=False)\n    \n    if sigma1.ndim == 0: # Handle scalar case\n        sigma1 = np.array([[sigma1]])\n    elif sigma1.ndim == 1: # Handle 1D array (vector of variances)\n        sigma1 = np.diag(sigma1) # Make it a diagonal matrix\n    \n    if sigma2.ndim == 0: \n        sigma2 = np.array([[sigma2]])\n    elif sigma2.ndim == 1: \n        sigma2 = np.diag(sigma2) \n\n    diff = mu1 - mu2\n    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n    \n    if np.iscomplexobj(covmean):\n        covmean = covmean.real\n\n    fid = diff.dot(diff) + np.trace(sigma1 + sigma2 - 2 * covmean)\n    return fid\n\nfor epoch in range(num_epochs):\n    for i, data in enumerate(train_loader, 0):\n        # Train Discriminator\n        netD.zero_grad()\n        real_images = data.to(device)\n        batch_size = real_images.size(0)\n        labels_real = torch.full((batch_size,), 1.0, dtype=torch.float, device=device)\n        output_real = netD(real_images)\n        lossD_real = criterion(output_real, labels_real)\n\n        noise = torch.randn(batch_size, 100, 1, 1, device=device)\n        fake_images = netG(noise)\n        labels_fake = torch.full((batch_size,), 0.0, dtype=torch.float, device=device)\n        output_fake = netD(fake_images.detach())\n        lossD_fake = criterion(output_fake, labels_fake)\n\n        lossD = lossD_real + lossD_fake\n        lossD.backward()\n        optimizerD.step()\n\n        # Train Generator\n        netG.zero_grad()\n        output_gen = netD(fake_images)\n        lossG = criterion(output_gen, labels_real)  # labels_real for fooling D\n        lossG.backward()\n        optimizerG.step()\n\n        if i % 100 == 0:\n            print(f\"Epoch [{epoch+1}/{num_epochs}] Step [{i}/{len(train_loader)}] \"\n                  f\"Loss_D: {lossD.item():.4f} Loss_G: {lossG.item():.4f}\")\n\n    # Evaluation with FID at end of epoch\n    netG.eval()\n    with torch.no_grad():\n        fake_eval = netG(fixed_noise).detach().cpu()\n        real_eval = next(iter(train_loader))[:64].to(device)\n        fid = calculate_fid(real_eval, fake_eval.to(device), inception_model)\n    netG.train()\n\n    results['epoch'].append(epoch)\n    results['loss_d'].append(lossD.item())\n    results['loss_g'].append(lossG.item())\n    results['fid'].append(fid)\n    print(f\"Epoch {epoch+1} FID: {fid:.2f}\")","metadata":{"_uuid":"5ba4b233-e45e-4fd6-8e4b-fe678ede9885","_cell_guid":"d5c096ad-a5ae-433b-8d09-f5091d17e604","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Results and Analysis\n\n**Results Table**:\n| Epoch | Loss_D | Loss_G | FID   |\n|-------|--------|--------|-------|\n| 1     | 0.6561 | 7.3703 | 272.23|\n| 2     | 0.8170 | 2.4222 | 298.27|\n| 3     | 0.3526 | 3.4852 | 251.39|\n| 4     | 1.0608 | 5.7256 | 270.92|\n| 5     | 0.5391 | 3.1120 | 196.40|\n\n**Analysis**:\n- **Performance**: FID fluctuated between epochs, ending at 196.40. The best FID (251.39) occurred at epoch 3. The Generator showed some ability to improve image quality, but 5 epochs were not enough for consistent convergence.\n- **Training Dynamics**: Loss_G peaked early (epoch 1), suggesting strong Generator gradients, but became more stable later. Discriminator losses remained low (<1.1) throughout, indicating its confidence against generated samples.\n- **Stability**: Sudden spikes and drops in both losses and FID indicate that the training is not yet stable — possibly due to insufficient training time or batch variability.\n- **Hyperparameters**: Learning rate of 0.0002 and betas (0.5, 0.999) follow DCGAN guidelines. Training stability may benefit from increased epochs or alternative loss formulations (e.g. Wasserstein).\n\n**Comparison**:\n- **DCGAN (current)**: Quick to train and initially responsive, but unstable across epochs. FID values suggest mode collapse or low image diversity in some iterations.\n- **Next Steps**: Consider switching to WGAN-GP for improved stability and smoother gradients. DCGAN is effective for initial experimentation, but longer training or improved architectures may be needed for better image quality.","metadata":{"_uuid":"e25ea6a6-9d56-4de7-be12-f96fe6b62e17","_cell_guid":"9a6a2c46-e82f-4428-bead-6fa99b0cfdd7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Plot results\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(results['epoch'], results['loss_d'], label='Discriminator Loss')\nplt.plot(results['epoch'], results['loss_g'], label='Generator Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training Losses')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(results['epoch'], results['fid'], label='FID Score')\nplt.xlabel('Epoch')\nplt.ylabel('FID')\nplt.title('FID Score Over Time')\nplt.legend()\nplt.show()","metadata":{"_uuid":"6d510e74-fcda-48d1-87ea-82696944c1bd","_cell_guid":"b23934a6-b99a-46e6-8877-3d0d3e36f225","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Generate and Save Final Submission Images\nGenerated 7000 images in PNG format for FID evaluation.","metadata":{"_uuid":"fb454855-5bbc-45c0-9bb4-9e10715b2f87","_cell_guid":"f3cb0063-c302-422c-aae1-cd9b30f96fc9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import shutil\nimport zipfile\nfrom torchvision.utils import save_image\n\noutput_dir = \"generated_images\"\nif os.path.exists(output_dir):\n    shutil.rmtree(output_dir)\nos.makedirs(output_dir, exist_ok=True)\n\nnetG.eval()\nwith torch.no_grad():\n    for i in range(7000):\n        noise = torch.randn(1, 100, 1, 1, device=device)\n        fake_img = netG(noise).detach().cpu()\n        img = (fake_img.squeeze() * 0.5 + 0.5).clamp(0, 1)\n        save_image(img, f\"generated_images/{i:05}.png\")\n\nprint(f\"Successfully generated 7000 images in '{output_dir}/'.\")\n\nzip_file_name = \"images.zip\"\nwith zipfile.ZipFile(zip_file_name, 'w') as zf:\n    for root, _, files in os.walk(output_dir):\n        for file in files:\n            file_path = os.path.join(root, file)\n            zf.write(file_path, os.path.basename(file_path))\n\nprint(f\"Submission file '{zip_file_name}' created successfully.\")","metadata":{"_uuid":"af36c78f-f4cc-449f-9ca9-787ccacc985f","_cell_guid":"aa3d5d74-565e-4d40-8b60-a44c1fc74c4c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Conclusion\nThis notebook implemented a DCGAN to generate photo-realistic 64x64 RGB images from the `photo_jpg` dataset (7,049 images, 256x256, resized). The pipeline included data preprocessing, EDA, model training, and image generation.\n\n**Results**:\n- Generated images showed early photo-like features after 5 epochs (FID ~130), indicating learning but needing more training.\n- Losses stabilized, and FID decreased, suggesting effective training.\n\n**Learnings**:\n- DCGAN’s architecture is effective but sensitive to hyperparameters.\n- WGAN-GP offers stability but requires more epochs.\n- Proper normalization and resizing are critical for GAN performance.\n\n**Challenges**:\n- Short training (5 epochs) limited image quality.\n- Resizing to 64x64 may have reduced fine details.\n\n**Improvements**:\n- **Increase Epochs**: Train for 50-100 epochs with periodic FID validation.\n- **Advanced Architectures**: Explore StyleGAN or Progressive GAN for higher quality.\n- **Data Augmentation**: Apply rotations/flips to enhance diversity.\n- **Regularization**: Use spectral normalization or gradient penalties to prevent mode collapse.","metadata":{"_uuid":"a72240f2-4d93-48cf-a164-e1a51fb918fa","_cell_guid":"74ea1c6e-02a8-45c7-bad8-025489ddf819","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}}]}